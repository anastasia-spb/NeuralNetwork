{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "**Perceptron** is the building block of neural networks,  and it's just an encoding of our equation into a small graph.\n",
    "We can represent perceptron in two ways: \n",
    "1. bias unit comes from an input node with a value of one\n",
    "2. bias is inside the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **higher weight** means the neural network considers that input **more important** than other inputs.\n",
    "The perceptron applies weights to the inputs and sums them in a process known as **linear combination.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron formula\n",
    "\n",
    "$$f(x_{1}, x_{2},..., x_{m}) = \\begin{cases}\n",
    "  0 \\text{ if } b + \\sum \\omega_{i} x_{i} < 0 \\\\    \n",
    "  1 \\text{ if } b + \\sum \\omega_{i} x_{i} \\ge 0    \n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First weights and bias are assigned a random value. Then they are updated using a learning algorithm line **gradient descent**.\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example AND (OR) perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -2.0                    0          Yes\n",
      "      0          1                  -1.0                    0          Yes\n",
      "      1          0                  -1.0                    0          Yes\n",
      "      1          1                   0.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -2.0 # -1.0 for OR perceptron or increase weights\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT Perceptron Example (NOT operation on the second input and ignores the first input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                   0.0                    1          Yes\n",
      "      0          1                  -1.0                    0          Yes\n",
      "      1          0                   0.0                    1          Yes\n",
      "      1          1                  -1.0                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.0\n",
    "weight2 = -1.0\n",
    "bias = 0.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Algorithm\n",
    "\n",
    "1. Random weights and bias\n",
    "2. For every misclassified point:\n",
    "    1. If prediction = 0: <br>\n",
    "       Change $\\omega_{i} + \\alpha x_{i}$ <br>\n",
    "       Change $b$ to $b + \\alpha$ <br>\n",
    "    2. If prediction = 1: <br>\n",
    "       Change $\\omega_{i} - \\alpha x_{i}$ <br>\n",
    "       Change $b$ to $b - \\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas # csv reading\n",
    "\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0]) # Matrix product of two arrays\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y_hat == 0 and y[i] == 1: #< point is classified negative, but it has a positive label\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y_hat == 1 and y[i] == 0: #< point is classified positive, but it has a negative label\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "        # If the point is correctly classified, do nothing\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1)) #random start weights\n",
    "    b = np.random.rand(1)[0] + x_max #random bias\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('data.csv', \n",
    "            names=['x1', 'x2', 'label']) #df == data frame\n",
    "# Usage example: print(df['x1'][1])\n",
    "x1 = df['x1'][:].as_matrix() \n",
    "x2 = df['x2'][:].as_matrix() \n",
    "labels = df['label'][:].as_matrix()\n",
    "X_mat = np.column_stack((x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_lines = trainPerceptronAlgorithm(X_mat, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error function - gradient descent\n",
    "\n",
    "1. Should be continuous\n",
    "2. Should be differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous activation function - > Sigmoid \n",
    "$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    result = 1 / (1 + np.exp(-x))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function\n",
    "\n",
    "$$P(\\text{class } i) = \\frac{e^{Z_{i}}}{e^{Z_{1}} + ... + e^{Z_{n}}}$$\n",
    "\n",
    "where $Z_{1}, ..., Z_{n}$ are linear function scores.\n",
    "\n",
    "Softmax for $ n = 2 $ values is the same as the sigmoid function under a certain condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    frac_bottom = np.sum(np.exp(L))\n",
    "    prob = np.divide(np.exp(L), frac_bottom)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Hot-Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "\n",
    "The model classifies most points correctly with P(all) indicating how accurate the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "Sums up negatives of logratithms of the probabilities is called the **cross entropy**.\n",
    "\n",
    "$$\\text{Cross-Entropy } = - \\sum^{m}_{i = 1} y_{i}\\ln(p_{i}) + (1 - y_{i})\\ln(1 - p_{i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    prob_1 = Y*np.log(P)\n",
    "    prob_2 = np.subtract(1, Y)*np.log(np.subtract(1, P))\n",
    "    return -np.sum(prob_1  +prob_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Cross entropy\n",
    "\n",
    "$$\\text{Cross-Entropy } = - \\sum_{i = 1}^{n} \\sum^{m}_{j = 1} y_{ij}\\ln(p_{ij})$$\n",
    "\n",
    "n - number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "* Take data\n",
    "* Pick a random model\n",
    "* Calculate error\n",
    "* Minimize the error and obtain better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "<figure>\n",
    " <img src=\"img_lectures/grad_descent.jpg\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "In summary, the gradient is\n",
    "$$\\nabla E = -(y - \\hat{y})(x_{1},...,x_{n}, 1)$$\n",
    "\n",
    "actually a scalar times the coordinates of the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one weight update can be calculated as:\n",
    "\n",
    "$$\\Delta \\omega_{i} = \\alpha \\delta x_{i}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $\\delta$ is the error term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.3775406687981454\n",
      "Amount of Error:\n",
      "0.1224593312018546\n",
      "Change in Weights:\n",
      "[0.0143892 0.0287784]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])\n",
    "\n",
    "# Calculate one gradient descent step for each weight\n",
    "nn_output = sigmoid(np.dot(x, w))\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "del_w = learnrate * error * nn_output * (1 - nn_output) * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron vs Gradient Descent\n",
    "\n",
    "### Gradient\n",
    "1. $\\omega_{i} to \\omega_{i} + \\alpha (y-\\hat{y})x_{i}$\n",
    "2. $\\hat{y}$ can take any value between zero and one\n",
    "3. if point is classified correctly -> go even farther away\n",
    "\n",
    "\n",
    "### Perceptron\n",
    "1. Only misclassified points change their weights (add $\\alpha x_{i}$ if positive; subtract $\\alpha x_{i}$ if negative)\n",
    "2. $\\hat{y}$ can take value of zero and one only\n",
    "3. if point is classified correctly -> do nothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "Linear model is a whole probability space. <br>\n",
    "Model is the linear combination of previous models times the weights plus some bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"img_lectures/NN_Arch.png\" width=\"1280\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "Feedforward is the process neural networks use to turn the input into an output.\n",
    "\n",
    "$$\\hat{y} = \\sigma W^{(2)} \\sigma W^{(1)}(x)$$\n",
    "\n",
    "#### Feed forwarding is composing a bunch of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector\n",
    "\n",
    "**magnitude + direction** <br>\n",
    "\n",
    "### Matrix\n",
    "\n",
    "**is a rectangular arrangement of numbers into rows and columns** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers.\n",
    "\n",
    "Things to do:\n",
    "\n",
    "* Calculate the input to the hidden layer.\n",
    "* Calculate the hidden layer output.\n",
    "* Calculate the input to the output layer.\n",
    "* Calculate the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[0.41492192 0.42604313 0.5002434 ]\n",
      "Output-layer Output:\n",
      "[0.49815196 0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "# Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Doing a feedforward operation.\n",
    "* Comparing the output of the model with the desired output.\n",
    "* Calculating the error.\n",
    "* Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "* Use this to update the weights, and get a better model.\n",
    "* Continue this until we have a model that is good.\n",
    "\n",
    "Gradient descent step:\n",
    "\n",
    "$$W^{'(k)}_{ij} \\leftarrow W^{(k)}_{ij} - \\alpha \\frac{\\partial E}{\\partial W^{(k)}_{ij}}$$\n",
    "\n",
    "#### Back propagation is taking the derivative at each piece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern in backpropagation flow\n",
    "\n",
    "\n",
    "**add** gate: gradient distributor\n",
    "**max** gate: gradient router\n",
    "\n",
    "\n",
    "### if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated \n",
    "(Source: https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ?\n",
    "Vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topologically sorted** <br>\n",
    "all inputs must come to every node before the output can be consumed (from left to right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationalGraph(object):\n",
    "    #...\n",
    "    def forward(inputs):\n",
    "        # 1. pass inputs to input gates\n",
    "        # 2. forward the computational graph\n",
    "        for gate in self.graph.nodes_topologically_sorted():\n",
    "            gate.forward()\n",
    "        return loss # the final gate in the graph outputs the loss\n",
    "    def backward():\n",
    "        for gate in reversed(self.graph.nodes_topologically_sorted()):\n",
    "            gate.backward() # chain rule applied\n",
    "        return input_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyGate(object): # or multiply layer\n",
    "    def forward(x,y):\n",
    "        self.x = x # is used in backward propagation calculation\n",
    "        se;f.y = y # is used in backward propagation calculation\n",
    "        z = x*y\n",
    "        return z\n",
    "    def backward(dz): # what is our gradient on the final loss, tell the influence on the end of the circuit\n",
    "        dx = self.y * dz # [dz/dx * dL/dz] ... L is loss function\n",
    "        dy = self.x * dz # [dz/dy * dL/dz]\n",
    "        return [dx, dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
