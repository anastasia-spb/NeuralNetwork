{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "The goal of this project is to design and implement the convolutional neural networks for traffic signs recognition. <br>\n",
    "Built model is trained on images from the German Traffic Sign Database.\n",
    "\n",
    "The steps of this project are the following:\n",
    "1. Load the data set. Explore, summarize and visualize the data set\n",
    "2. Design, train and test a model architecture\n",
    "3. Use the model to make predictions on new images\n",
    "4. Analyze the softmax probabilities of the new images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "There are three datasets provided: training, validation and testing.\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image.\n",
    "\n",
    "---\n",
    "\n",
    "Number of training examples is **34799**; <br>\n",
    "Number of testing examples is **12630**; <br>\n",
    "Number of validation examples is **4410**; <br>\n",
    "Image data shape is **32x32**; <br>\n",
    "Trere are **43** classes. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes names and corresponding indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ClassId                                           SignName\n",
      "0         0                               Speed limit (20km/h)\n",
      "1         1                               Speed limit (30km/h)\n",
      "2         2                               Speed limit (50km/h)\n",
      "3         3                               Speed limit (60km/h)\n",
      "4         4                               Speed limit (70km/h)\n",
      "5         5                               Speed limit (80km/h)\n",
      "6         6                        End of speed limit (80km/h)\n",
      "7         7                              Speed limit (100km/h)\n",
      "8         8                              Speed limit (120km/h)\n",
      "9         9                                         No passing\n",
      "10       10       No passing for vehicles over 3.5 metric tons\n",
      "11       11              Right-of-way at the next intersection\n",
      "12       12                                      Priority road\n",
      "13       13                                              Yield\n",
      "14       14                                               Stop\n",
      "15       15                                        No vehicles\n",
      "16       16           Vehicles over 3.5 metric tons prohibited\n",
      "17       17                                           No entry\n",
      "18       18                                    General caution\n",
      "19       19                        Dangerous curve to the left\n",
      "20       20                       Dangerous curve to the right\n",
      "21       21                                       Double curve\n",
      "22       22                                         Bumpy road\n",
      "23       23                                      Slippery road\n",
      "24       24                          Road narrows on the right\n",
      "25       25                                          Road work\n",
      "26       26                                    Traffic signals\n",
      "27       27                                        Pedestrians\n",
      "28       28                                  Children crossing\n",
      "29       29                                  Bicycles crossing\n",
      "30       30                                 Beware of ice/snow\n",
      "31       31                              Wild animals crossing\n",
      "32       32                End of all speed and passing limits\n",
      "33       33                                   Turn right ahead\n",
      "34       34                                    Turn left ahead\n",
      "35       35                                         Ahead only\n",
      "36       36                               Go straight or right\n",
      "37       37                                Go straight or left\n",
      "38       38                                         Keep right\n",
      "39       39                                          Keep left\n",
      "40       40                               Roundabout mandatory\n",
      "41       41                                  End of no passing\n",
      "42       42  End of no passing by vehicles over 3.5 metric ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.read_csv('signnames.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes representatives\n",
    "\n",
    "<figure>\n",
    " <img src=\"img_write_up/ClassExamples.png\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes distribution in training, validation and testing sets\n",
    "\n",
    "<figure>\n",
    " <img src=\"img_write_up/OriginHist.png\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) was taken as a base.\n",
    "\n",
    "The final validation set accuracy is **0.95**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fake data\n",
    "\n",
    "If the amount of class samples is less than mean value. Then for every sample in this class several new samples will be generated to fill the gap between original and mean numbers. <br>\n",
    "Two ways of transformation were implemented. First, rotate image by random angle in range Â±10. Then translate this image. <br>\n",
    "Second method applyes affine tranformaion by randomly choosing source and destination points in certain range. <br>\n",
    "In both cases the image brightness can be also changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"img_write_up/20_orig.png\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img src=\"img_write_up/AugmentExample.png\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For following classes were generated additional samples:\n",
    "\n",
    "* For class 0 generate 4 new images based on each image. Total 720.\n",
    "* For class 6 generate 2 new images based on each image. Total 720.\n",
    "* For class 14 generate 1 new images based on each image. Total 690.\n",
    "* For class 15 generate 1 new images based on each image. Total 540.\n",
    "* For class 16 generate 2 new images based on each image. Total 720.\n",
    "* For class 19 generate 4 new images based on each image. Total 720.\n",
    "* For class 20 generate 2 new images based on each image. Total 600.\n",
    "* For class 21 generate 2 new images based on each image. Total 540.\n",
    "* For class 22 generate 2 new images based on each image. Total 660.\n",
    "* For class 23 generate 1 new images based on each image. Total 450.\n",
    "* For class 24 generate 3 new images based on each image. Total 720.\n",
    "* For class 26 generate 1 new images based on each image. Total 540.\n",
    "* For class 27 generate 3 new images based on each image. Total 630.\n",
    "* For class 28 generate 1 new images based on each image. Total 480.\n",
    "* For class 29 generate 3 new images based on each image. Total 720.\n",
    "* For class 30 generate 2 new images based on each image. Total 780.\n",
    "* For class 31 generate 1 new images based on each image. Total 690.\n",
    "* For class 32 generate 3 new images based on each image. Total 630.\n",
    "* For class 33 generate 1 new images based on each image. Total 599.\n",
    "* For class 34 generate 2 new images based on each image. Total 720.\n",
    "* For class 36 generate 2 new images based on each image. Total 660.\n",
    "* For class 37 generate 4 new images based on each image. Total 720.\n",
    "* For class 39 generate 2 new images based on each image. Total 540.\n",
    "* For class 40 generate 2 new images based on each image. Total 600.\n",
    "* For class 41 generate 3 new images based on each image. Total 630.\n",
    "* For class 42 generate 3 new images based on each image. Total 630."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes distribution in augmented training set\n",
    "\n",
    "<figure>\n",
    " <img src=\"img_write_up/AugmentHist.png\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set\n",
    "\n",
    "The image data are normalized so that the data has mean zero and equal variance. For image data, `(pixel - 128)/ 128` is a quick way to approximately normalize the data. <br>\n",
    "\n",
    "Then data were converted to grayscale. The color information can be ignored in this task due to the reason there are no two traffic signs which differ only by color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "<p></p> \n",
    "\n",
    "<figure>\n",
    " <img src=\"img_write_up/Architecture.jpg\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "The model architecture is similar to **Lenet**. It has three convolutional layers and three fully connected layers.\n",
    "\n",
    "| Layer | Description |\n",
    "| --- | --- |\n",
    "| Input | 32x32x1 Grayscale image |\n",
    "| Convolution 5x5 | 1x1 stride, 'SAME' padding, output 32x32x32, Leaky ReLU |\n",
    "| Max pooling | window size 2, output 16x16x32 |\n",
    "| Convolution 5x5 | 1x1 stride, 'SAME' padding, output 16x16x64, Leaky ReLU |\n",
    "| Max pooling | window size 2, output 8x8x64 |\n",
    "| Convolution 5x5 | 1x1 stride, 'SAME' padding, output 8x8x128, Leaky ReLU |\n",
    "| Max pooling | window size 2, output 4x4x128 |\n",
    "| Fully connected | Input 2048, output 1024, Leaky ReLU |\n",
    "| Dropout | Drop rate 0.5 |\n",
    "| Fully connected | Input 1024, output 340, Leaky ReLU |\n",
    "| Dropout | Drop rate 0.5 |\n",
    "| Fully connected | Input 340, output 43 |\n",
    "| Softmax |  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrafficSignsNet(features, dropout_rate, is_training):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    alpha = 0.01 # leaky ReLU parameter\n",
    "    padding = 'SAME'\n",
    "    \n",
    "    ##############################################################################################\n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 32x32x32.\n",
    "    conv1_K = 32\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, conv1_K), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(conv1_K))\n",
    "    conv1   = tf.nn.conv2d(features, conv1_W, strides=[1, 1, 1, 1], padding=padding) + conv1_b\n",
    "    \n",
    "    # Activation. Leaky ReLU\n",
    "    conv1 = tf.nn.leaky_relu(conv1, alpha)\n",
    "\n",
    "\n",
    "    # Max Pooling. Input = 32x32x32. Output = 16x16x32.\n",
    "    conv1 = max_pool(conv1, 2, name='conv1', padding=padding)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Layer 2: Convolutional. Input = 16x16x32 Output = 16x16x64.\n",
    "    conv2_K = 64\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, conv1_K, conv2_K), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(conv2_K))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding=padding) + conv2_b\n",
    "    \n",
    "    # Activation. Leaky ReLU\n",
    "    conv2 = tf.nn.leaky_relu(conv2, alpha)\n",
    "\n",
    "\n",
    "    # Max Pooling. Input = 16x16x64. Output = 8x8x64.\n",
    "    conv2 = max_pool(conv2, 2, name='conv2', padding=padding)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Layer 3: Convolutional. Input = 8x8x64 Output = 8x8x128.\n",
    "    conv3_K = 128\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, conv2_K, conv3_K), mean = mu, stddev = sigma))\n",
    "    conv3_b = tf.Variable(tf.zeros(conv3_K))\n",
    "    conv3   = tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding=padding) + conv3_b\n",
    "    \n",
    "    # Activation. Leaky ReLU\n",
    "    conv3 = tf.nn.leaky_relu(conv3, alpha)\n",
    "\n",
    "    # Max Pooling. Input = 8x8x128. Output = 4x4x128.\n",
    "    conv3 = max_pool(conv3, 2, name='conv3', padding=padding)\n",
    "    \n",
    "    ##############################################################################################\n",
    "    # Flatten. Input = 4x4x128. Output = 2048.\n",
    "    fc0   = flatten(conv3)\n",
    "    fc0_out = fc0.get_shape().as_list()[1]\n",
    "    \n",
    "    ##############################################################################################\n",
    "    # Layer 4: Fully Connected. Input = 2048. Output = 1024.\n",
    "    fc1_out = 1024\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(fc0_out, fc1_out), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(fc1_out))\n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(fc0, fc1_W), fc1_b, name='fc1')\n",
    "    \n",
    "    # Activation.\n",
    "    fc1    = tf.nn.leaky_relu(fc1, alpha)\n",
    "    # Dropout\n",
    "    fc1 = tf.layers.dropout(inputs=fc1, rate=dropout_rate, training=is_training)\n",
    "    \n",
    "    # Layer 5: Fully Connected. Input = 1024. Output = 340.\n",
    "    fc2_out = 340\n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(fc1_out, fc2_out), mean = mu, stddev = sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(fc2_out))\n",
    "    \n",
    "    fc2 = tf.add(tf.matmul(fc1, fc2_W), fc2_b, name='fc2')\n",
    "    \n",
    "    # Activation.\n",
    "    fc2    = tf.nn.leaky_relu(fc2, alpha)\n",
    "    # Dropout\n",
    "    fc2 = tf.layers.dropout(inputs=fc2, rate=dropout_rate, training=is_training)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # Layer 6: Fully Connected. Input = 340. Output = 43.\n",
    "    fc3_out = 43\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(fc2_out, fc3_out), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(fc3_out))\n",
    "    \n",
    "    logits = tf.add(tf.matmul(fc2, fc3_W), fc3_b, name='fc3')\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` is a placeholder for a batch of input images. <br>\n",
    "`y` is a placeholder for a batch of output labels. <br>\n",
    "`drop_rate` is a dropout rate, between 0 and 1. E.g. \"rate=0.1\" would drop out 10% of input units.  <br>\n",
    "`is_training` indicates whether to return the output in training mode (apply dropout) or in inference mode (return the input untouched).\n",
    "\n",
    "---\n",
    "\n",
    "`learn_rate` is one of the most important hyperparameters. Decides 'how rough' the weights are updated during training. This parameter is set to **0.001**. <br>\n",
    "\n",
    "Softmax cross entropy between logits and labels is computed by using *tf.nn.softmax_cross_entropy_with_logits_v2* function. The result is provided as an input to *tf.reduce_mean* function. It is used to compute the total cost. <br>\n",
    "As an oprimizer the **Adam optimization algorithm** is used. It is an extension to stochastic gradient descent and one of the most used optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set is used to assess how well the model is performing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Epoch | Validation Accuracy | --- | Epoch | Validation Accuracy |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 1 | 0.621 | --- | 21 | 0.950 |\n",
    "| 2 | 0.869 | --- | 22 | 0.957 |\n",
    "| 3 | 0.911 | --- | 23 | 0.959 |\n",
    "| 4 | 0.915 | --- | 24 | 0.956 |\n",
    "| 5 | 0.933 | --- | 25 | 0.958 |\n",
    "| 6 | 0.933 | --- | 26 | 0.956 |\n",
    "| 7 | 0.939 | --- | 27 | 0.952 |\n",
    "| 8 | 0.937 | --- | 28 | 0.967 |\n",
    "| 9 | 0.945 | --- | 29 | 0.960 |\n",
    "| 10 | 0.951 | --- | 30 | 0.960 |\n",
    "| 11 | 0.944 | --- | 31 | 0.961 |\n",
    "| 12 | 0.947 | --- | 32 | 0.962 |\n",
    "| 13 | 0.944 | --- | 33 | 0.956 |\n",
    "| 14 | 0.947 | --- | 34 | 0.965 |\n",
    "| 15 | 0.961 | --- | 35 | 0.968 |\n",
    "| 16 | 0.957 | --- | 36 | 0.967 |\n",
    "| 17 | 0.951 | --- | 37 | 0.946 |\n",
    "| 18 | 0.954 | --- | 38 | 0.960 |\n",
    "| 19 | 0.962 | --- | 39 | 0.963 |\n",
    "| 20 | 0.958 | --- | 40 | 0.966 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation accuracy plot through epochs\n",
    "\n",
    "<figure>\n",
    " <img src=\"img_write_up/ValidAccuracyThroughEpochs.png\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "5 german traffic signs images were downloaded. Images were normalized and converted to grayscale. Sign Type was correcty predicted for every image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"img_write_up/NewImages.png\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*traffic_signs_net* accurace on testing test set is **0.950**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the new images, the model's softmax probabilities to show the **certainty** of the model's predictions were printed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"img_write_up/ImgToEval.png\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    " \n",
    "---\n",
    "    \n",
    "<figure>\n",
    " <img src=\"img_write_up/ImgEval.png\" alt=\"Combined Image\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All signs were classified correctly.\n",
    "\n",
    "| Idx | Description | Probability | --- | Idx | Description | Probability | --- | Idx | Description | Probability |\n",
    "| --- | --- | --- | --- | --- | --- | --- |  --- | --- | --- | --- |\n",
    "| 5 | **Speed limit (80km/h)** | 9.9984431e-01 |  --- | 22 | **Bumpy road** | 1.0000000e+00 |  --- | 9 | **No passing** | 1.0000000e+00 |\n",
    "| 3 | Speed limit (60km/h) | 1.5569913e-04 |  --- | 29 | Bicycles crossing | 6.4850363e-26 | --- | 15 | No vehicles | 9.8773351e-27 |\n",
    "| 16 | Stop | 3.1646842e-14 |  --- |  26 | Traffic signals | 3.5302702e-32 | --- |  14 | Stop | 3.0215690e-18 |\n",
    "| 2 | Speed limit (50km/h) | 3.1258351e-17 | --- | 25 | Road work | 1.8902869e-33 | --- | 16 | Vehicles over 3.5 metric tons prohibited | 1.7943873e-22 |\n",
    "| 8 | Speed limit (120km/h) | 5.0107557e-18 | --- | 23 | Slippery road | 4.0317657e-35 | --- | 41 | End of no passing | 4.0317657e-35 |\n",
    "\n",
    "| Idx | Description | Probability | --- | Idx | Description | Probability |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 25 | **Road work** | 1.0000000e+00 |  --- | 23 | **Slippery road** | 9.9994075e-01 |\n",
    "| 18 | General caution | 9.5928652e-17 |  --- | 30 | Beware of ice/snow | 5.9071997e-05 |\n",
    "| 11 | Right-of-way at the next intersection | 1.0563458e-18 |  --- |  21 | Double curve | 1.2561594e-07 |\n",
    "| 30 | Beware of ice/snow | 3.1563438e-22 | --- | 19 | Dangerous curve to the left | 3.5005552e-08 |\n",
    "| 20 | Dangerous curve to the right | 1.9739832e-23 | --- | 10 | No passing for vehicles over 3.5 metric tons | 4.1987342e-09 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "The main parts of the task were:\n",
    "* random image generator for training database extension;\n",
    "* design and implement the network architecture;\n",
    "* tune hyperparameters.\n",
    "\n",
    "For designed network following hyperparameters had to be set:\n",
    "\n",
    "* learning rate : 0.001\n",
    "* drop rate : 0.5\n",
    "* batch size : 128\n",
    "* epochs : 40\n",
    "\n",
    "Also:\n",
    "\n",
    "* type of padding : 'SAME'\n",
    "* stride : 1\n",
    "* convolution filter size : 5x5\n",
    "* size of intermediate fully connected layers: 1024, 340\n",
    "* coefficient for leaky ReLU : 0.01\n",
    "* pooling strategy : max pooling\n",
    "\n",
    "\n",
    "Initial weights and bias were initialized by tf.truncated_normal function. This is the recommended initializer for neural network weights and filters. <br>\n",
    "Leaky ReLU activation function was chosen in order to prevent dead ReLU and initialize ReLU neurons with slightly positive biases. <br>\n",
    "\n",
    "Deep neural network learning process requires a lot of tuning hyperparameters. Due to high level of freedom it's almost impossible to manully try all combinations. In further research it would be interesting to try Hyperopt functionality, although it requires a lot of computational power. <br>\n",
    "\n",
    "Also an interesting observation: Many articles recommend to replace dropout by batch normalization. Or add batch normalization between every convolutional layer before or after pooling. In this case it gave worse results: 0.3 after 5th epoch. Drop out gave us 0.933 after 5th epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4 (Optional): Visualize the Neural Network's State with Test Images\n",
    "\n",
    "Feature maps for first and second convolutional layers are plotted below.<br>\n",
    "\n",
    "From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st convolutional layer**\n",
    "\n",
    "<figure>\n",
    " <img src=\"img_write_up/FeatMap.png\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "--- \n",
    "\n",
    "**2nd convolutional layer**\n",
    "\n",
    "<figure>\n",
    " <img src=\"img_write_up/FeatMapConv2.png\" alt=\"Combined Image\" />\n",
    "</figure>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
